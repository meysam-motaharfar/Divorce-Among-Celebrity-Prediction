{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d0131d",
   "metadata": {},
   "source": [
    "# 1) Data Scraping and Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef94cfd",
   "metadata": {},
   "source": [
    " In this note notebook, we scrape the data for celebrities, namely actors, actress, directors, comedians, singers, musicians, dancers, models, writers, playwrights, photographers, journalists and etc. from wikipedia. The purpose of this projects is to figure out the divorce rate and also life expectancy among celebrities. We also attempt to run the model to predict the divorce and life expectancy of celebrities based on the extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f106b2",
   "metadata": {},
   "source": [
    "Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238187a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import time\n",
    "import concurrent.futures\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f9171",
   "metadata": {},
   "source": [
    "Defining required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99322eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HTML_content(url):\n",
    "    \n",
    "    '''This function extracts the HTML content for a particular url'''\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    \n",
    "    if html.status_code == 200:\n",
    "        return bs4.BeautifulSoup(html.text, 'lxml')\n",
    "    elif html.status_code == 404:\n",
    "        return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_all_subcategories_for_each_category(url):\n",
    "    \n",
    "    '''This function gets the url for a category and return \n",
    "        entire subcategories and titles'''\n",
    "    \n",
    "    url_base = \"https://en.wikipedia.org\"\n",
    "    soup = get_HTML_content(url)\n",
    "    subcategories = soup.find_all('div', class_=\"CategoryTreeSection\")\n",
    "    \n",
    "    url_subcategories = [url_base + subcategory.find('a').get('href') for subcategory in subcategories]\n",
    "    titles_subcategories = [subcategory.find('a').text for subcategory in subcategories]\n",
    "    \n",
    "    return url_subcategories, titles_subcategories\n",
    "\n",
    "def get_all_pages_for_each_subcategory(url, title):\n",
    "    \n",
    "    '''This function extract urls and titles for all pages for each subcategories'''\n",
    "    \n",
    "    urls_subcategories_all_pages = []\n",
    "    urls_subcategories_all_pages.append(url)\n",
    "    url_base = \"https://en.wikipedia.org\"\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        row_next = None\n",
    "        soup = get_HTML_content(url)\n",
    "        rows = soup.find( 'div', id = \"mw-pages\").find_all(title = f\"Category:{title}\")\n",
    "        \n",
    "        for row in rows: \n",
    "            if 'next page' in row.text:\n",
    "                row_next = row\n",
    "            else:\n",
    "                continue\n",
    "        try: \n",
    "            href = row_next.get('href')\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "        url = url_base + href\n",
    "        urls_subcategories_all_pages.append(url)\n",
    "        \n",
    "    return urls_subcategories_all_pages\n",
    "\n",
    "def get_all_subcategories(url):\n",
    "    \n",
    "    '''This function extracts all urls for each category including those \n",
    "        subcategories which have multi-pages'''\n",
    "\n",
    "    urls_subcategories_all = []\n",
    "    urls_subcategories, titles_subcategories = get_all_subcategories_for_each_category(url)\n",
    "    for i in range(len(urls_subcategories)):\n",
    "        try:\n",
    "            urls_subcategories_all_pages = get_all_pages_for_each_subcategory(urls_subcategories[i], titles_subcategories[i])\n",
    "            for j in range(len(urls_subcategories_all_pages)):\n",
    "                urls_subcategories_all.append(urls_subcategories_all_pages[j])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return urls_subcategories_all\n",
    "\n",
    "def get_the_link_for_profiles(urls):\n",
    "    \n",
    "    '''This function extract links and also the name for each celebrity'''\n",
    "    \n",
    "    links = []\n",
    "    names = []\n",
    "    \n",
    "    for url in urls:\n",
    "        soup = get_HTML_content(url)\n",
    "        \n",
    "        try:\n",
    "            rows = soup.find('div', {'id': 'mw-pages'}).find_all('li')\n",
    "            url_base = \"https://en.wikipedia.org\"\n",
    "            \n",
    "            for i in range(len(rows)):\n",
    "                href = rows[i].find('a').get('href')\n",
    "                name = rows[i].find('a').text.split('(', 1)[0]\n",
    "                link = url_base + href\n",
    "                \n",
    "                if link not in links or name not in names:\n",
    "                    links.append(link)\n",
    "                    names.append(name)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return links, names\n",
    "\n",
    "def get_the_data_from_table(soup, name):\n",
    "    \n",
    "    url_base = \"https://en.wikipedia.org\" \n",
    "    \n",
    "    date_formats = [ r\"\\b[A-Za-z]+\\s\\d{1,2},\\s\\d{4}\\b\",   \n",
    "                r\"\\d{1,2}\\s[A-Za-z]+\\s\\d{4}\", \n",
    "                r\"\\b\\d{4}\\b\"]\n",
    "    \n",
    "    row_born = None\n",
    "    row_died = None\n",
    "    row_spouse = None\n",
    "    row_children = None\n",
    "\n",
    "    try:\n",
    "        rows = soup.find('table', class_ = 'infobox biography vcard').find_all('tr')\n",
    "\n",
    "        # Here we get the corresponding infromation about the Birth, Death, Spouse and Children from table of content\n",
    "\n",
    "        for row in rows:\n",
    "            if row.text and re.search(r'Born', row.text.split()[0]):\n",
    "                row_born = row\n",
    "            elif row.text and re.search(r'Died', row.text.split()[0]):\n",
    "                row_died = row\n",
    "            elif row.text and re.search(r'Spouse', row.text.split()[0]):\n",
    "                row_spouse = row\n",
    "            elif row.text and re.search(r'Children', row.text.split()[0]):\n",
    "                row_children = row\n",
    "                break\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        match = ''\n",
    "        match = row_born.find(class_ = 'nickname').text\n",
    "        if match != '':\n",
    "            name = match\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        match = None\n",
    "        for date_format in date_formats:\n",
    "            match = re.search(date_format, str(row_born))\n",
    "            if match: \n",
    "                date_of_birth = match.group()\n",
    "                break \n",
    "        else:\n",
    "            date_of_birth = np.nan \n",
    "    except:\n",
    "        date_of_birth = np.nan\n",
    "\n",
    "    try:\n",
    "        place_of_birth = row_born.find(class_ = 'birthplace').text\n",
    "    except:\n",
    "        place_of_birth = np.nan\n",
    "\n",
    "    try:\n",
    "        match = None\n",
    "        for date_format in date_formats:\n",
    "            match = re.search(date_format, str(row_died))\n",
    "            if match:\n",
    "                date_of_death = match.group()\n",
    "                break\n",
    "        else:\n",
    "            date_of_death = np.nan \n",
    "    except: \n",
    "        date_of_death = np.nan\n",
    "\n",
    "    try:\n",
    "        place_of_death = row_died.find(class_ = 'deathplace').text\n",
    "    except: \n",
    "        place_of_death = np.nan\n",
    " \n",
    "    try:\n",
    "        name_of_spouse = row_spouse.find('td').text.split('(', 1)[0].strip().replace('\\n', '')\n",
    "    except:\n",
    "        name_of_spouse = np.nan\n",
    "        \n",
    "    try:  \n",
    "        matches = None\n",
    "        text = row_spouse.find_all(class_ = 'marriage-display-ws')[0].text\n",
    "        matches = re.findall(r'\\d{4}', text)\n",
    "\n",
    "        if matches:\n",
    "            date_of_marriage = matches[0]\n",
    "        else:\n",
    "            date_of_marriage = np.nan\n",
    "\n",
    "        keywords = ['died', 'death']\n",
    "\n",
    "        if len(matches)==2 and not any(keyword in text for keyword in keywords):\n",
    "            date_of_divorce = matches[1]\n",
    "        else:\n",
    "            date_of_divorce = np.nan\n",
    "\n",
    "    except:\n",
    "        date_of_marriage = np.nan\n",
    "        date_of_divorce = np.nan\n",
    "\n",
    "    try:\n",
    "        nom = None\n",
    "        nom = len(row_spouse.find_all(class_ = 'marriage-display-ws'))\n",
    "        if nom !=0:\n",
    "            number_of_marriage = nom\n",
    "        else:\n",
    "            number_of_marriage=1\n",
    "    except:\n",
    "        number_of_marriage = np.nan\n",
    "\n",
    "    try:\n",
    "        matches = None\n",
    "        marriages_end_in_death = None\n",
    "\n",
    "        marriages_end_in_death = row_spouse.text.count('died') + row_spouse.text.count('death')\n",
    "\n",
    "        last_marriage = row_spouse.find_all(class_ = 'marriage-display-ws')[number_of_marriage-1].text\n",
    "\n",
    "        matches = re.findall(r'\\d{4}', last_marriage)\n",
    "\n",
    "        keywords = ['div', 'sep', 'ann', 'divorced', 'divorce', 'annulled', 'separated', 'died', 'death']\n",
    "\n",
    "        if len(matches)==2 or any(keyword in last_marriage for keyword in keywords):\n",
    "            number_of_divorce = number_of_marriage - marriages_end_in_death\n",
    "        else: \n",
    "            number_of_divorce = number_of_marriage - marriages_end_in_death-1\n",
    "\n",
    "    except:\n",
    "        marriages_end_in_death = np.nan\n",
    "        number_of_divorce = np.nan\n",
    "\n",
    "    try:\n",
    "        number_of_children = None\n",
    "        number_of_children = row_children.find('td').text.split('[', 1)[0]\n",
    "    except:\n",
    "        number_of_children = np.nan\n",
    "\n",
    "    try:\n",
    "        href_spouse = None\n",
    "        soup_spouse = None\n",
    "\n",
    "        if any(words in row_spouse.find_all('a')[0].get('href') for words in name_of_spouse.replace('\\u200b', '').split()[1:]):\n",
    "            href_spouse = row_spouse.find_all('a')[0].get('href')\n",
    "\n",
    "        if href_spouse:\n",
    "            link_spouse = url_base + href_spouse\n",
    "            soup_spouse = get_HTML_content(link_spouse)\n",
    "    except: \n",
    "        soup_spouse = None\n",
    "        \n",
    "    return row_born, row_died, row_spouse, row_children, name, date_of_birth, place_of_birth, date_of_death, place_of_death, date_of_marriage, date_of_divorce, number_of_marriage, number_of_divorce, number_of_children, marriages_end_in_death, name_of_spouse, soup_spouse\n",
    "\n",
    "\n",
    "def get_data_for_celebrity_and_spouse(links, names, sex, profession):\n",
    "    \n",
    "    '''This function get the list of links and names of celebrities and return Name (full name), \n",
    "        Date_of_Birth, Place_of_Birth, Date_of_Death, Place_of_Death, Name_of_Spouse, Date_of_Marriage,\n",
    "        Date_of_Divorce, Number_of_Marriage, Number_of_Divorce, Date_of_Birth_Spouse, Place_of_Birth_Spouse,\n",
    "        Date_of_Death_Spouse, Place_of_Death_Spouse, Number_of_Marriage_Spouse, Number_of_Divorce_Spouse,\n",
    "        Number_of_Children_Spouse, Name_of_Spouse_Spouse as single dataframe'''\n",
    "\n",
    "    Name = []\n",
    "    Date_of_Birth = [] \n",
    "    Place_of_Birth = []\n",
    "    Date_of_Death = []\n",
    "    Place_of_Death = []\n",
    "    Name_of_Spouse = []\n",
    "    Number_of_Marriage = []\n",
    "    Number_of_Divorce = []\n",
    "    Number_of_Children = []\n",
    "    Date_of_Marriage = []\n",
    "    Date_of_Divorce = []\n",
    "    Marriages_End_in_Death = []\n",
    "    Date_of_Birth_Spouse = []\n",
    "    Place_of_Birth_Spouse = []\n",
    "    Date_of_Death_Spouse = []\n",
    "    Place_of_Death_Spouse = []\n",
    "    Number_of_Marriage_Spouse = []\n",
    "    Number_of_Divorce_Spouse = []\n",
    "    Number_of_Children_Spouse = []\n",
    "    Name_of_Spouse_Spouse = []\n",
    "    \n",
    "    # Here we iterate over all the links\n",
    "\n",
    "    for i in range(len(links)):\n",
    "        \n",
    "        # we implement retry mechanism to overcome the limiting rate for sending request\n",
    "        \n",
    "        max_retries = 3\n",
    "        retry = 0\n",
    "        soup = None\n",
    "        \n",
    "        while retry < max_retries:\n",
    "            \n",
    "            try:\n",
    "                soup = get_HTML_content(links[i])\n",
    "                if soup is not None:\n",
    "                    break\n",
    "            except:\n",
    "                retry+=1\n",
    "                time.sleep(5)\n",
    "\n",
    "        row_born, row_died, row_spouse, row_children, name, date_of_birth, place_of_birth, date_of_death, place_of_death, date_of_marriage, date_of_divorce, number_of_marriage, number_of_divorce, number_of_children, marriages_end_in_death, name_of_spouse, soup_spouse = get_the_data_from_table(soup, names[i])\n",
    "        row_born_spouse, row_died_spouse, row_spouse_spouse, row_children_spouse, name_of_spouse, date_of_birth_spouse, place_of_birth_spouse, date_of_death_spouse, place_of_death_spouse, date_of_marriage_spouse, date_of_divorce_spouse, number_of_marriage_spouse, number_of_divorce_spouse, number_of_children_spouse, marriages_end_in_death_spouse, name_of_spouse_spouse, soup_spouse_spouse = get_the_data_from_table(soup_spouse, name_of_spouse)\n",
    "        \n",
    "          \n",
    "        Name.append(name)\n",
    "        Date_of_Birth.append(date_of_birth) \n",
    "        Place_of_Birth.append(place_of_birth)\n",
    "        Date_of_Death.append(date_of_death)\n",
    "        Place_of_Death.append(place_of_death)\n",
    "        Name_of_Spouse.append(name_of_spouse)\n",
    "        Number_of_Marriage.append(number_of_marriage)\n",
    "        Number_of_Divorce.append(number_of_divorce)\n",
    "        Number_of_Children.append(number_of_children)\n",
    "        Date_of_Marriage.append(date_of_marriage)\n",
    "        Date_of_Divorce.append(date_of_divorce)\n",
    "        Marriages_End_in_Death.append(marriages_end_in_death)\n",
    "        Date_of_Birth_Spouse.append(date_of_birth_spouse) \n",
    "        Place_of_Birth_Spouse.append(place_of_birth_spouse)\n",
    "        Date_of_Death_Spouse.append(date_of_death_spouse)\n",
    "        Place_of_Death_Spouse.append(place_of_death_spouse)\n",
    "        Number_of_Marriage_Spouse.append(number_of_marriage_spouse)\n",
    "        Number_of_Divorce_Spouse.append(number_of_divorce_spouse)\n",
    "        Number_of_Children_Spouse.append(number_of_children_spouse)\n",
    "        Name_of_Spouse_Spouse.append(name_of_spouse_spouse)\n",
    "\n",
    "\n",
    "    data = {'Name': Name, \n",
    "        'Date_of_Birth': Date_of_Birth,\n",
    "        'Place_of_Birth': Place_of_Birth,\n",
    "        'Date_of_Death' : Date_of_Death,\n",
    "        'Place_of_Death': Place_of_Death,\n",
    "        'Name_of_Spouse':  Name_of_Spouse,\n",
    "        'Date_of_Marriage': Date_of_Marriage,\n",
    "        'Date_of_Divorce': Date_of_Divorce,\n",
    "        'Number_of_Marriage': Number_of_Marriage,\n",
    "        'Number_of_Divorce': Number_of_Divorce,\n",
    "        'Number_of_Children': Number_of_Children,\n",
    "        'Marriages_End_in_Death': Marriages_End_in_Death,\n",
    "        'Date_of_Birth_Spouse': Date_of_Birth_Spouse,\n",
    "        'Place_of_Birth_Spouse': Place_of_Birth_Spouse,\n",
    "        'Date_of_Death_Spouse': Date_of_Death_Spouse,\n",
    "        'Place_of_Death_Spouse': Place_of_Death_Spouse,\n",
    "        'Number_of_Marriage_Spouse': Number_of_Marriage_Spouse,\n",
    "        'Number_of_Divorce_Spouse' : Number_of_Divorce_Spouse,\n",
    "        'Number_of_Children_Spouse': Number_of_Children_Spouse,\n",
    "        'Name_of_Spouse_Spouse': Name_of_Spouse_Spouse,\n",
    "        'Sex': sex,\n",
    "        'Profession': profession\n",
    "       }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    try:\n",
    "        df[\"Date_of_Birth\"] = pd.to_datetime(df[\"Date_of_Birth\"], errors='coerce')\n",
    "        df[\"Date_of_Death\"] = pd.to_datetime(df[\"Date_of_Death\"], errors='coerce')\n",
    "        df[\"Date_of_Marriage\"] = pd.to_datetime(df[\"Date_of_Marriage\"], errors='coerce')\n",
    "        df[\"Date_of_Divorce\"] = pd.to_datetime(df[\"Date_of_Divorce\"], errors='coerce')\n",
    "        df[\"Date_of_Birth_Spouse\"] = pd.to_datetime(df[\"Date_of_Birth_Spouse\"], errors='coerce')\n",
    "        df[\"Date_of_Death_Spouse\"] = pd.to_datetime(df[\"Date_of_Death_Spouse\"], errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return df\n",
    "\n",
    "def scraper(url, sex, profession):\n",
    "\n",
    "    '''This is the scraper to extract all data into one single dataframe by giving \n",
    "        the url for category'''\n",
    "\n",
    "    urls = get_all_subcategories(url)\n",
    "    links, names = get_the_link_for_profiles(urls)\n",
    "    data = get_data_for_celebrity_and_spouse(links[:100], names[:100], sex, profession)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def scraper_with_multithreading(url_female, url_male, profession):\n",
    "    \n",
    "    '''This function use multithreading to scrape data for both male and female celebrity \n",
    "        by providing male and female urls for each category'''\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_female = executor.submit(scraper, url_female, 'F', profession)\n",
    "        future_male = executor.submit(scraper, url_male, 'M', profession)\n",
    "        \n",
    "    female_data = future_female.result()\n",
    "    male_data = future_male.result()\n",
    "    \n",
    "    data = pd.concat([female_data, male_data])\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scraper_with_multithreading_using_links(links_female, names_female, links_male, names_male, profession):\n",
    "    \n",
    "    '''This function also use multithreading to scrape data for both male and female celebrity\n",
    "        by providing links and names'''\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_female = executor.submit(get_data_for_celebrity_and_spouse, links_female, names_female, 'F', profession)\n",
    "        future_male = executor.submit(get_data_for_celebrity_and_spouse, links_male, names_male, 'M', profession)\n",
    "        \n",
    "    female_data = future_female.result()\n",
    "    male_data = future_male.result()\n",
    "    \n",
    "    data = pd.concat([female_data, male_data])\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94166686",
   "metadata": {},
   "source": [
    "## 1.1) Scraping data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d41bf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1955.9075269699097\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:Female_models_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:Male_models_by_nationality\"\n",
    "models = scraper_with_multithreading(url_female, url_male, 'model')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ce50271",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.to_csv('Models.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13b0fc",
   "metadata": {},
   "source": [
    "## 1.2) Scraping data for dancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a3c030",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554.0225760936737\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:Female_dancers_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:Male_dancers_by_nationality\"\n",
    "dancers = scraper_with_multithreading(url_female, url_male, 'dancer')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "204d2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dancers.to_csv('Dancers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8340db8",
   "metadata": {},
   "source": [
    "## 1.3) Scraping data for journalists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a77a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2804.695547103882\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:Women_journalists_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:Male_journalists_by_nationality\"\n",
    "journalists = scraper_with_multithreading(url_female, url_male, 'journalist')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f37e2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "journalists.to_csv('Journalists.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f574d",
   "metadata": {},
   "source": [
    "## 1.4) Scraping data for actors and actress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a4469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9560.27544093132\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:Film_actresses_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:Male_film_actors_by_nationality\"\n",
    "actors_actress = scraper_with_multithreading(url_female, url_male, 'actor/actress')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd0f94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_actress.to_csv('Actors-actress.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74593f7f",
   "metadata": {},
   "source": [
    "## 1.5) Scraping data for singers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f814366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3048.709326028824\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:21st-century_women_singers_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:21st-century_male_singers_by_nationality\"\n",
    "singers = scraper_with_multithreading(url_female, url_male, 'singer')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e603f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "singers.to_csv('Singers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fc812",
   "metadata": {},
   "source": [
    "## 1.6) Scraping data for writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9af272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3611.731799840927\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:21st-century_women_writers_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:21st-century_male_singers_by_nationality\"\n",
    "writers = scraper_with_multithreading(url_female, url_male, 'writers')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "314a4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "writers.to_csv('Writers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3aefa",
   "metadata": {},
   "source": [
    "## 1.7) Scraping data for musicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e06720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4816.020472049713\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:21st-century_women_writers_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:21st-century_male_singers_by_nationality\"\n",
    "musicians = scraper_with_multithreading(url_female, url_male, 'musician')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb4ff210",
   "metadata": {},
   "outputs": [],
   "source": [
    "musicians.to_csv('Musicians.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752c52e",
   "metadata": {},
   "source": [
    "## 1.8) Scraping data for comedians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c7f71a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929.846773147583\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:Women_comedians_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:Male_comedians_by_nationality\"\n",
    "comedians = scraper_with_multithreading(url_female, url_male, 'comedians')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4916384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comedians.to_csv('Comedians.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c035e",
   "metadata": {},
   "source": [
    "## 1.9) Scraping data for playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95d3b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091.908532857895\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = \"https://en.wikipedia.org/wiki/Category:Women_dramatists_and_playwrights_by_nationality\"\n",
    "url_male = \"https://en.wikipedia.org/wiki/Category:Male_dramatists_and_playwrights_by_nationality\"\n",
    "playwrights = scraper_with_multithreading(url_female, url_male, 'playwrights')\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5c17765",
   "metadata": {},
   "outputs": [],
   "source": [
    "playwrights.to_csv('Playwrights.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c07cc0",
   "metadata": {},
   "source": [
    "## 1.10) Scraping data for directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eac1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3753.587508916855\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = 'https://en.wikipedia.org/wiki/Category:Women_film_directors'\n",
    "url_total = 'https://en.wikipedia.org/wiki/Category:Film_directors_by_nationality'\n",
    "urls = get_all_subcategories(url_total)\n",
    "links_total, names_total = get_the_link_for_profiles(urls)\n",
    "\n",
    "urls = get_all_subcategories(url_female)\n",
    "links_female, names_female = get_the_link_for_profiles(urls)\n",
    "\n",
    "links_male = []\n",
    "names_male = []\n",
    "\n",
    "for index, (link, name) in enumerate(zip(links_total, names_total)):\n",
    "    if link not in links_female:\n",
    "        links_male.append(link)\n",
    "        names_male.append(name)\n",
    "        \n",
    "directors = scraper_with_multithreading_using_links(links_female, names_female, links_male, names_male, 'director')\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c59d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directors.to_csv('Directors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1682c8",
   "metadata": {},
   "source": [
    "## 1.11) Scraping data for photographers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68a51b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1342.5910720825195\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_female = 'https://en.wikipedia.org/wiki/Category:Women_photographers_by_nationality'\n",
    "url_total = 'https://en.wikipedia.org/wiki/Category:Photographers_by_nationality'\n",
    "urls = get_all_subcategories(url_total)\n",
    "links_total, names_total = get_the_link_for_profiles(urls)\n",
    "\n",
    "urls = get_all_subcategories(url_female)\n",
    "links_female, names_female = get_the_link_for_profiles(urls)\n",
    "\n",
    "links_male = []\n",
    "names_male = []\n",
    "\n",
    "for index, (link, name) in enumerate(zip(links_total, names_total)):\n",
    "    if link not in links_female:\n",
    "        links_male.append(link)\n",
    "        names_male.append(name)\n",
    "        \n",
    "photographers = scraper_with_multithreading_using_links(links_female, names_female, links_male, names_male, 'director')\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "003c1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "photographers.to_csv('Photographers.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
